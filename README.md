# ğŸš€ GPTCacheÂ StreamingÂ Clustering
*Adaptive Embedding Caching for Large LanguageÂ Models*

[![License](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](LICENSE)
[![PythonÂ 3.10+](https://img.shields.io/badge/Python-3.10%2B-yellow.svg)](https://www.python.org/)
[![CondaÂ Environment](https://img.shields.io/badge/Conda-env.yaml-success)](env.yaml)

> **â€œGPTCacheÂ StreamingÂ Clusteringâ€** - a streamingâ€‘clustering cache policy for LLM embeddings built on top ofÂ [GPTCache](https://github.com/zilliztech/GPTCache).

---

## ğŸ“Œ Overview  
This repository implements a **streaming clustering** caching policy that groups similar embeddings into dynamically updated clusters.  
The result: **lower cacheâ€‘lookup latency** and **higher throughput** for LLMâ€‘powered applications.

---

## ğŸ“‚ Directory Layout
    â”œâ”€â”€ env.yaml                     # Reproducible Conda environment
    â”œâ”€â”€ download_models.py           # Oneâ€‘click model & dataset fetcher
    â”œâ”€â”€ benchmark_clustering_speedup.py  # Synthetic benchmark (clustering vs FAISS)
    â”œâ”€â”€ gptcache_benchmark.py        # Realâ€‘world benchmark (AGÂ News, CNN/DailyMail)
    â”œâ”€â”€ results/                     # All benchmark outputs live here
    â””â”€â”€ README.md                    # You are here ğŸ“–

---

## ğŸ› ï¸Â Setup

### 1ï¸âƒ£Â Clone & enter the repo
    git clone https://github.com/uzielroy/GPTCache-Streaming-Clustering.git
    cd GPTCache-Streaming-Clustering

### 2ï¸âƒ£Â Create the Conda environment
    conda env create -f env.yaml
    conda activate gptcache-env

### 3ï¸âƒ£Â Download models & datasets
    python download_models.py

*Tip:* To use a local HF cache or mirror, edit `download_models.py`.

---

## ğŸ§ª Synthetic Benchmark â€“ `benchmark_clustering_speedup.py`

| Argument             | Description                     | Default          |
|----------------------|---------------------------------|------------------|
| `--n_clusters_list`  | List of cluster counts          | `500 1000 2000`  |
| `--per_cluster_list` | Points per cluster              | `50 100`         |
| `--dim`              | Embedding dimension             | `128`            |
| `--tau`              | Cluster similarity threshold    | `0.75`           |
| `--alpha`            | EMA centroid update rate        | `0.1`            |
| `--n_queries`        | Query count (hitsâ€¯+â€¯misses)      | `3000`           |
| `--seed`             | RNG seed                        | `0`              |

Example:
---
    python benchmark_clustering_speedup.py \
      --n_clusters_list 500 1000 \
      --per_cluster_list 50 100 \
      --dim 128 --tau 0.75 --alpha 0.1 \
      --n_queries 3000 --seed 42

---

## ğŸ“ˆ Realâ€‘Data Benchmark â€“ `gptcache_benchmark.py`

| Argument            | Description                                    | Default                      |
|---------------------|------------------------------------------------|------------------------------|
| `--models`          | LLM backâ€‘ends (`phi2`, `tinyllama`, `falcon`, â€¦) | `phi2 tinyllama falcon`     |
| `--embedders`       | Embedding models (`bge-large`, `minilm`, â€¦)     | `bge-large minilm`          |
| `--thrs`            | Cache similarity threshold                      | `0.68`                      |
| `--caches`          | Cache sizes                                     | `3000`                      |
| `--epochs`          | Dataset passes                                  | `1`                         |
| `--seed`            | RNG seed                                        | `42`                        |
| `--cluster_cache`   | Enable streaming clustering (flag)              | disabled                    |
| `--tau_inter`       | Interâ€‘cluster threshold                         | `0.5`                       |
| `--tau_intra`       | Intraâ€‘cluster threshold                         | `0.63`                      |
| `--cluster_alpha`   | EMA centroid rate                               | `0.1`                       |

Clusteredâ€‘cache example:
---
    python gptcache_benchmark.py \
      --models phi2 tinyllama \
      --embedders bge-large \
      --thrs 0.68 --caches 1000 \
      --cluster_cache \
      --tau_inter 0.75 --tau_intra 0.65 \
      --cluster_alpha 0.1 \
      --epochs 1 --seed 42

---

## ğŸ“ŠÂ Output Layout
    results/
    â””â”€â”€ <run_tag>/
        â”œâ”€â”€ requests.csv                 # Perâ€‘request log
        â”œâ”€â”€ latency_throughput_stats.txt # Latency & throughput summary
        â”œâ”€â”€ latency_cdf.png              # CDF of request latencies
        â”œâ”€â”€ hit_rate.png                 # Running hitâ€‘rate graph
        â””â”€â”€ cluster_stats.txt            # Cluster diagnostics
    benchmark_summary.csv                # Aggregated summary across runs

---

### ğŸ—‚ï¸Â Output File Descriptions

| File | What it contains |
|------|------------------|
| **`requests.csv`** | One row per request. Columns: `latency_ms` (endâ€‘toâ€‘end latency), `hit` (1Â = cache hit, 0Â = miss), `gpu_mb` (GPU memory at request time), `ram_mb` (host RAM), plus sequential `idx`. |
| **`latency_throughput_stats.txt`** | Plainâ€‘text summary of latency statistics (`avg_latency`, `median_latency`, `p95`, `p99`) and overall throughput (`throughput_qps`), along with totals for `hits`, `misses`, and final accuracy/score. |
| **`latency_cdf.png`** | Cumulative Distribution Function plot of perâ€‘request latencies-quick visual insight into tailâ€‘latency behaviour. |
| **`hit_rate.png`** | Line chart of cumulative cache hitâ€‘rate over time (only produced when caching is enabled). |
| **`cluster_stats.txt`** | Final snapshot of clustering state: total clusters, nonâ€‘empty clusters, meanÂ & max cluster size, plus a histogram of cluster sizes. A running log (`cluster_stats_progress.txt`) is also appended to during long runs. |
| **`benchmark_summary.csv`** | Aggregated runâ€‘level metrics-each row corresponds to a single run tag and includes model, workload, cache settings, accuracy, latency percentiles, throughput, memory usage, and clustering stats. |


