# ğŸš€ GPTCacheÂ StreamingÂ Clustering
*Adaptive Embedding Caching for Large LanguageÂ Models*

[![License](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](LICENSE)
[![PythonÂ 3.10+](https://img.shields.io/badge/Python-3.10%2B-yellow.svg)](https://www.python.org/)
[![CondaÂ Environment](https://img.shields.io/badge/Conda-env.yaml-success)](env.yaml)

> **â€œGPTCacheÂ StreamingÂ Clusteringâ€** - a streamingâ€‘clustering cache policy for LLM embeddings built on top ofÂ [GPTCache](https://github.com/zilliztech/GPTCache).

---

## ğŸ“Œ Overview  
This repository implements a **streaming clustering** caching policy that groups similar embeddings into dynamically updated clusters.  
The result: **lower cacheâ€‘lookup latency** and **higher throughput** for LLMâ€‘powered applications.

---

## ğŸ“‚ Directory Layout
    â”œâ”€â”€ env.yaml                     # Reproducible Conda environment
    â”œâ”€â”€ download_models.py           # Oneâ€‘click model & dataset fetcher
    â”œâ”€â”€ benchmark_clustering_speedup.py  # Synthetic benchmark (clustering vs FAISS)
    â”œâ”€â”€ gptcache_benchmark.py        # Realâ€‘world benchmark (AGÂ News, CNN/DailyMail)
    â”œâ”€â”€ results/                     # All benchmark outputs live here
    â””â”€â”€ README.md                    # You are here ğŸ“–

---

## ğŸ› ï¸Â Setup

### 1ï¸âƒ£Â Clone & enter the repo
    git clone https://github.com/uzielroy/GPTCache-Streaming-Clustering.git
    cd GPTCache-Streaming-Clustering

### 2ï¸âƒ£Â Create the Conda environment
    conda env create -f env.yaml
    conda activate gptcache-env

### 3ï¸âƒ£Â Download models & datasets
    python download_models.py

*Tip:* To use a local HF cache or mirror, edit `download_models.py`.

---

## ğŸ§ª Synthetic Benchmark â€“ `benchmark_clustering_speedup.py`

| Argument             | Description                     | Default          |
|----------------------|---------------------------------|------------------|
| `--n_clusters_list`  | List of cluster counts          | `500 1000 2000`  |
| `--per_cluster_list` | Points per cluster              | `50 100`         |
| `--dim`              | Embedding dimension             | `128`            |
| `--tau`              | Cluster similarity threshold    | `0.75`           |
| `--alpha`            | EMA centroid update rate        | `0.1`            |
| `--n_queries`        | Query count (hitsâ€¯+â€¯misses)      | `3000`           |
| `--seed`             | RNG seed                        | `0`              |

Example:
---

python benchmark_clustering_speedup.py \
  --n_clusters_list 500 1000 \
  --per_cluster_list 50 100 \
  --tau_list 0.75 \
  --alpha_list 0.1 \
  --dim 128 \
  --n_queries 3000 \
  --seed 42
  
---

### ğŸ—‚ï¸Â Synthetic Benchmark Outputs (`benchmark_clustering_speedup.py`)

| File / Folder | What it contains |
|---------------|------------------|
| **`results_summary.csv`** | Master spreadsheet with one row per `(per_cluster, n_clusters, Ï„, Î±)` configuration. Columns include: baseline and clustered latency statistics (`flat_mean_us`, `cluster_median_us`, `cluster_p95_us` â€¦), hitâ€‘rate, inter/intra latency means, and speedâ€‘up ratios. |
| **`plots/`** | All visualisations are written here.<br><br>â€¢ **`cdf_pc*_nc*_tau*_alpha*.png`** â€“ Empirical CDF of perâ€‘request latencies for that configuration (flat vs. clustered).<br>â€¢ **`hitrate_pc*_nc*_tau*_alpha*.png`** â€“ Cumulative cache hitâ€‘rate curve for the same configuration.<br>â€¢ **`breakdown_pc*_nc*_tau*_alpha*.png`** â€“ Bar chart comparing *interâ€‘cluster* vs. *intraâ€‘cluster* mean latencies.<br>â€¢ **`ablation_latency_vs_tau.png`** â€“ Line plot: clustered mean latency vsâ€¯Ï„ for each Î± value.<br>â€¢ **`ablation_hitrate_vs_tau.png`** â€“ Line plot: hitâ€‘rate vsâ€¯Ï„ for each Î± value.<br>â€¢ **`ablation_latency_vs_alpha.png`** â€“ Line plot: clustered mean latency vsâ€¯Î± for each Ï„ value.<br>â€¢ **`ablation_hitrate_vs_alpha.png`** â€“ Line plot: hitâ€‘rate vsâ€¯Î± for each Ï„ value. |
| **Console stdout** | For every sweep combination the script prints: <br>â€¢ Progress bars for insertion and lookâ€‘ups.<br>â€¢ Perâ€‘run dictionary of latency stats and hitâ€‘rate.<br>â€¢ A final **SUMMARY TABLE** listing all configurations with baseline/clustered latencies, hitâ€‘rate, and speedâ€‘up. <br>Redirect stdout to a file if you wish to keep it: <br>`python benchmark_clustering_speedup.py ... > synthetic_bench.log` |



## ğŸ“ˆ Realâ€‘Data Benchmark â€“ `gptcache_benchmark.py`

| Argument            | Description                                    | Default                      |
|---------------------|------------------------------------------------|------------------------------|
| `--models`          | LLM backâ€‘ends (`phi2`, `tinyllama`, `falcon`, â€¦) | `phi2 tinyllama falcon`     |
| `--embedders`       | Embedding models (`bge-large`, `minilm`, â€¦)     | `bge-large minilm`          |
| `--thrs`            | Cache similarity threshold                      | `0.68`                      |
| `--caches`          | Cache sizes                                     | `3000`                      |
| `--epochs`          | Dataset passes                                  | `1`                         |
| `--seed`            | RNG seed                                        | `42`                        |
| `--cluster_cache`   | Enable streaming clustering (flag)              | disabled                    |
| `--tau_inter`       | Interâ€‘cluster threshold                         | `0.5`                       |
| `--tau_intra`       | Intraâ€‘cluster threshold                         | `0.63`                      |
| `--cluster_alpha`   | EMA centroid rate                               | `0.1`                       |

Clusteredâ€‘cache example:
---
    python gptcache_benchmark.py \
      --models phi2 tinyllama \
      --embedders bge-large \
      --thrs 0.68 --caches 1000 \
      --cluster_cache \
      --tau_inter 0.75 --tau_intra 0.65 \
      --cluster_alpha 0.1 \
      --epochs 1 --seed 42

---

## ğŸ“ŠÂ Output Layout
    results/
    â””â”€â”€ <run_tag>/
        â”œâ”€â”€ requests.csv                 # Perâ€‘request log
        â”œâ”€â”€ latency_throughput_stats.txt # Latency & throughput summary
        â”œâ”€â”€ latency_cdf.png              # CDF of request latencies
        â”œâ”€â”€ hit_rate.png                 # Running hitâ€‘rate graph
        â””â”€â”€ cluster_stats.txt            # Cluster diagnostics
    benchmark_summary.csv                # Aggregated summary across runs

---

### ğŸ—‚ï¸Â Output File Descriptions

| File | What it contains |
|------|------------------|
| **`requests.csv`** | One row per request. Columns: `latency_ms` (endâ€‘toâ€‘end latency), `hit` (1Â = cache hit, 0Â = miss), `gpu_mb` (GPU memory at request time), `ram_mb` (host RAM), plus sequential `idx`. |
| **`latency_throughput_stats.txt`** | Plainâ€‘text summary of latency statistics (`avg_latency`, `median_latency`, `p95`, `p99`) and overall throughput (`throughput_qps`), along with totals for `hits`, `misses`, and final accuracy/score. |
| **`latency_cdf.png`** | Cumulative Distribution Function plot of perâ€‘request latencies-quick visual insight into tailâ€‘latency behaviour. |
| **`hit_rate.png`** | Line chart of cumulative cache hitâ€‘rate over time (only produced when caching is enabled). |
| **`cluster_stats.txt`** | Final snapshot of clustering state: total clusters, nonâ€‘empty clusters, meanÂ & max cluster size, plus a histogram of cluster sizes. A running log (`cluster_stats_progress.txt`) is also appended to during long runs. |
| **`benchmark_summary.csv`** | Aggregated runâ€‘level metrics-each row corresponds to a single run tag and includes model, workload, cache settings, accuracy, latency percentiles, throughput, memory usage, and clustering stats. |


